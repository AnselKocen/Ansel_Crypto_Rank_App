from datetime import datetime, timedelta
from pathlib import Path

from crypto_project_pipeline import (
    build_week_dirs,
    stage1_etl,
    stage1_load_news,
    stage1_clean_text,
    stage2_feature_engineering,
    stage2_sentiment,
    merge_sentiment_feature
)

BASE_DIR = Path(__file__).resolve().parent
# ==============================================
#  Step 0: è‡ªåŠ¨ç¡®å®šæœ€è¿‘çš„å‘¨ä¸‰ï¼ˆè¿”å› datetimeï¼‰
# ==============================================
def get_latest_wednesday(today: datetime) -> datetime:
    days_since_wed = (today.weekday() - 2) % 7
    return today - timedelta(days=days_since_wed)

# ==============================================
#  Step 1: å¢é‡æ›´æ–° df_mergedï¼ˆåªä¸‹è½½æ–°çš„ä¸€å‘¨ï¼‰
# ==============================================
def update_df_merged(api_key: str, history_path: str, output_root = BASE_DIR.name):
    notice_list = []
    today = datetime.today()
    latest_wed = get_latest_wednesday(today)  #  ä¿æŒä¸º datetime ç±»å‹
    import pandas as pd
    import numpy as np
    from sklearn.linear_model import ElasticNet
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    import joblib
    # æœ€æ–°çš„è¦ä¿è¯æˆ‘ä»¬ä¼ å…¥çš„dfæ˜¯æ²¡æœ‰ret_lead1çš„
    # è¯»å–å†å²æ•°æ®å¹¶æ£€æŸ¥æ˜¯å¦å·²æ›´æ–°
    df_hist = pd.read_csv(history_path)
    df_hist["date"] = pd.to_datetime(df_hist["date"])
    last_date = df_hist["date"].max().date()
    next_day = last_date + timedelta(days=1)

    data_dir = BASE_DIR
    fig_dir = BASE_DIR  # å¦‚æœåç»­éœ€è¦å›¾åƒï¼Œå°±ç»Ÿä¸€æ”¾åœ¨ BASE_DIR

    print(f"###NOTICE### Latest Date in Historical Data: {last_date}ï¼ŒLatest Wednesday: {latest_wed.date()}")
    notice_list.append(f"Latest Date in Historical Data: {last_date}ï¼ŒLatest Wednesday: {latest_wed.date()}")
    if last_date >= latest_wed.date():
        print("Data already includes the most recent week. No update needed.")
        notice_list.append("Data already includes the most recent week. No update needed.")
        return df_hist.copy(), latest_wed, Path(history_path).parent,notice_list

    #elif today.weekday() == 2:  # Wednesday
        #notice_list.append("Today is Wednesday(UTC)! Please wait until Thursday to ensure complete data.")
        #return df_hist.copy(), latest_wed, Path(history_path).parent,notice_list
    # === å¦‚æœä»Šå¤©æ˜¯å‘¨ä¸‰ï¼Œä¸”å†å²æ•°æ®å·²ç»åŒ…å«äº†ä¸Šä¸€å‘¨çš„å®Œæ•´æ•°æ®ï¼Œåˆ™ä¸æ›´æ–°
    elif today.weekday() == 2 and last_date >= (latest_wed - timedelta(days=7)).date():
        print("Today is Wednesday (UTC), and last weekâ€™s data has already been fully updated. Please wait until Thursday to get the latest weekâ€™s complete data.")
        notice_list.append(
            "Today is Wednesday (UTC), and last weekâ€™s data has already been fully updated. Please wait until Thursday to get the latest weekâ€™s complete data.")
        return df_hist.copy(), latest_wed, Path(history_path).parent, notice_list

    # === æ‹‰å–æœ€è¿‘çš„ä»·æ ¼æ•°æ®å¹¶æ„å»º market ç‰¹å¾ ===
    stage1_etl(
        api_key=api_key,
        pages=[1,2],
        top_limit=100,
        history_limit=110,# ä¸æ”¹å¦‚æœçœŸçš„å¾ˆä¹…æ²¡æœ‰æŠ“ä»–çš„è¯ 120å¤©å°±ä¸å¤Ÿäº† è¿™ä¸ªæˆ‘ä»¬ä¹‹åå†æƒ³åŠæ³•
        currency="USD",
        data_dir=data_dir
    )
    # å»é™¤ latest_wed ä¹‹åçš„æ•°æ®ï¼Œé¿å…å‡ºç°æœªæ¥è¡Œæƒ…
    #df_prices = pd.read_csv(data_dir / "stage_1_crypto_data.csv")
    #df_prices.columns = [c.strip().lower() for c in df_prices.columns]

    #df_prices.columns = [col.strip().lower() for col in df_prices.columns]
    #print(f"æ˜¯å¦åŒ…å« 'date'ï¼š{'date' in df_prices.columns}")
    print('-------------------------------------------------------------------------------------------------------')

    #df_prices["date"] = pd.to_datetime(df_prices["date"])
   # df_prices = df_prices[df_prices["date"] <= latest_wed]
    #print(df_prices)
    #print("ğŸ“… df_prices æœ€å¤§æ—¥æœŸï¼š", df_prices["date"].max())
    last_date_df = pd.to_datetime(last_date)
    df_prices = pd.read_csv(BASE_DIR / "stage_1_crypto_data.csv")
    #print(last_date_df)
    df_features = stage2_feature_engineering(df_prices, data_dir)

    print(df_features)
    df_features["date"] = pd.to_datetime(df_features["date"]).dt.date  # è½¬ä¸º date æ–¹ä¾¿æ¯”è¾ƒ
    print(df_features)
    df_features = df_features[df_features["date"] > last_date]
    print(df_features)

    # === æŠ“å–æ–°é—»å¹¶è®¡ç®— sentiment ç‰¹å¾ === ä¿®æ”¹äº†æŠ“å–æ—¶é—´
    #news_start_date = latest_wed - timedelta(days=6)
    #news_end_date = latest_wed + timedelta(days=1)
    print(last_date)
    print(latest_wed)
    #news_start_date = last_date + timedelta(days=1)
    news_start_date = datetime.combine(last_date + timedelta(days=1), datetime.min.time())
    news_end_date = latest_wed + timedelta(days=1)# +1 æ˜¯ä¸ºäº†åŒ…å«å‘¨ä¸‰
    print(f"æŠ“å–æ–°é—»æ—¶é—´èŒƒå›´: {news_start_date} è‡³ {news_end_date}")
    df_news = stage1_load_news(api_key, news_start_date, news_end_date, data_dir)
    df_clean = stage1_clean_text(df_news, data_dir)
    stage2_sentiment(df_clean, data_dir, fig_dir)

    # æ›´æ–°è¯äº‘
    # from crypto_project_pipeline import generate_sentiment_wordclouds
    # generate_sentiment_wordclouds(df_sent, fig_dir, latest_wed)
    # å­˜åˆ°æ›´æ–°è·¯å¾„
    #renew_path = BASE_DIR / "figures"
    #generate_sentiment_wordclouds(df_sent, renew_path, latest_wed)
    # === åˆå¹¶ market å’Œ sentiment ç‰¹å¾ ===
    df_weekly_sent = pd.read_csv(data_dir / "stage_2_sentiment_weekly.csv")
    df_new = merge_sentiment_feature(df_features, df_weekly_sent, save_path=data_dir)
    #df_new["ret_lead1"] = df_new.groupby("symbol")["return"].shift(-1) #ä¸åº”è¯¥åœ¨è¿™é‡Œmergeret_lead1ï¼Œåˆ é™¤

    # === åˆå¹¶åˆ°å†å²æ•°æ®å¹¶ä¿å­˜ ===
    df_combined = pd.concat([df_hist, df_new], ignore_index=True)
    df_combined = df_combined.drop_duplicates(subset=["date", "symbol"]).sort_values("date")
    print(f"âš å³å°†è¦†ç›–å†å²æ–‡ä»¶ï¼š{history_path}ï¼Œä¿å­˜æœ€æ–°çš„ df_mergedï¼Œå…± {df_combined['date'].nunique()} å‘¨æ•°æ®ã€‚")
    df_combined.to_csv(history_path, index=False)
    # ç”Ÿæˆä»ªè¡¨ç›˜
    # from crypto_project_pipeline import draw_fear_greed_gauge_from_latest
    # df_latest = pd.read_csv(BASE_DIR / "df_merged_history.csv")
    # draw_fear_greed_gauge_from_latest(df_latest, fig_dir / "fear_greed_gauge1.png")
    # æ›´æ–°ä»ªè¡¨ç›˜
    #renew_path = BASE_DIR/"figures"
    #draw_fear_greed_gauge_from_latest(df_latest, renew_path / "fear_greed_gauge1.png")
    return df_combined, latest_wed, data_dir,notice_list




# ==============================================
# Step 2: æ¨¡å‹è®­ç»ƒ + é¢„æµ‹ï¼ˆåªé¢„æµ‹æœ€æ–°ä¸€å‘¨ï¼‰
# ==============================================
def run_prediction_pipeline(api_key: str, history_path: str):
    df_merged, latest_wed, data_dir,notice_list = update_df_merged(api_key, history_path)
    df_merged = df_merged[df_merged["date"] <= latest_wed].copy()
    import pandas as pd
    import numpy as np
    from sklearn.linear_model import ElasticNet
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    import joblib

    weeks = sorted(df_merged["date"].unique())
    train_weeks = weeks[-53:-1]  # 52å‘¨è®­ç»ƒ
    test_week = weeks[-1]        # æœ€æ–°ä¸€å‘¨é¢„æµ‹

    # åº”è¯¥åœ¨è¿™é‡ŒåŠ ä¸€å¥ï¼Œä¸ºæˆ‘ä»¬è¯»å–çš„df_merged æ„å»ºret_lead1ï¼Œåç»­ éƒ½æ˜¯æ­£å¸¸çš„æ“ä½œï¼Œä¸éœ€è¦æ›´æ”¹äº†ã€‚ä¸‹é¢è¿™æ®µæ˜¯æ–°æ·»åŠ çš„
    df_merged["ret_lead1"] = (
        df_merged.sort_values(["symbol", "date"])
        .groupby("symbol")["return"]
        .shift(-1)
    )

    df_train = df_merged[df_merged["date"].isin(train_weeks)].dropna(subset=["ret_lead1"])
    df_test = df_merged[df_merged["date"] == test_week]

    feature_sets = {
        "all": [col for col in df_train.columns if col not in ["date", "symbol", "return", "ret_lead1", "open", "high", "low", "close"]],
        "market": [col for col in df_train.columns if any(k in col for k in ["momentum", "volatility", "usd_volume", "base_volume", "return_sign", "long_candle", "strev"])]
    }

    fixed_params = {
        "enet_all": {"alpha": 0.01, "l1_ratio": 0.5},
        "enet_market": {"alpha": 0.01, "l1_ratio": 0.5},
        "extra_all": {"n_estimators": 200, "max_depth": 10, "max_features": "sqrt"},
        "extra_market": {"n_estimators": 200, "max_depth": 10, "max_features": "sqrt"}
    }

    results = {}

    for model_key in ["enet", "extra"]:
        for tag in ["all", "market"]:
            name = f"{model_key}_{tag}"
            print(f"\n æ­£åœ¨è®­ç»ƒæ¨¡å‹ {name} ...")

            X_train = df_train[feature_sets[tag]].copy()
            y_train = df_train["ret_lead1"].fillna(0)
            X_test = df_test[feature_sets[tag]].copy()

            imputer = SimpleImputer(strategy="mean")
            X_train = imputer.fit_transform(X_train)
            X_test = imputer.transform(X_test)

            if model_key == "enet":
                model = ElasticNet(max_iter=10000, random_state=42, **fixed_params[name])
            else:
                model = ExtraTreesRegressor(random_state=42, **fixed_params[name])

            pipe = Pipeline([
                ("scale", StandardScaler()),
                ("model", model)
            ])

            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)

            df_out = df_test[["symbol"]].copy()
            df_out["y_pred"] = y_pred
            df_out = df_out.sort_values("y_pred", ascending=False)

            results[name] = df_out
            out_path = data_dir / f"pred_{name}.csv"
            df_out.to_csv(out_path, index=False)
            print(f" {name} è¾“å‡ºä¿å­˜è‡³ {out_path}")

    # === æ‰“å°æ¨èå¸ç§ + æ„é€ è¿”å›ç»“æœ ===
    results_dict = {}

    for name, df in results.items():
        print(f"\n æ¨¡å‹: {name}")
        print("Top 20 æ¨èå¸ç§:")
        print(df.head(20).to_string(index=False))
        print("Bottom 20 åšç©ºå»ºè®®:")
        print(df.tail(20).to_string(index=False))

        top_list = df.head(20)["symbol"].tolist()
        bot_list = df.tail(20)["symbol"].tolist()

        print(f">>>TOP20_{name}: " + ",      ".join(top_list))
        print(f">>>BOT20_{name}: " + ",      ".join(bot_list))

        results_dict[name] = {
            "top": top_list,
            "bot": bot_list
        }
    return results_dict, notice_list


def run_for_client(api_key: str, history_path: str):
    return run_prediction_pipeline(api_key, history_path)


if __name__ == "__main__":
    API_KEY = ""
    HISTORY_PATH = BASE_DIR / "df_merged_history.csv"
    run_for_client(API_KEY, str(HISTORY_PATH))

